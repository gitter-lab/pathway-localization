MODELS=['LinearNN', 'SimpleGCN', 'GATCONV', 'GIN2']
#MODELS=['LinearNN', 'SimpleGCN', 'GIN2']
#MODELS=['LinearNN', 'GIN2']
SKLEARN_MODELS=['logit','rf']
PGM_MODELS=['TrainedPGM','NaivePGM']
#NETWORKS=['allReactomePaths','allPathBank']
NETWORKS=['allReactomePaths','allPathBank']
#NETWORKS=['somePathBank']
FEATURES=['comPPINodes','compartmentsNodes']
#FEATURES=['comPPINodes']
#FEATURES=['comPPINodes','compartmentsNodes','mergedKeyWords_5']
#FEATURES=['compartmentsNodes']


#The PGM code is tricky to install and uses c++, so it's not currently automatable. 
#Right now we just assume you have the compiled code in the right places. 
rule pgm_runs:
    input:
        net="data/{networks}.txt",
        feat="data/{features}.tsv"
    params:
        curModel = "{pgm_model}",
        oDir = "pgm_runs/pgm_{pgm_model}-{networks}-{features}"
    output:
        full = "pgm_runs/pgm_{pgm_model}-{networks}-{features}.p"
    shell:
        "bash runPGM.sh {input.net} {input.feat} {params.curModel} {output.full} {params.oDir};"

#Right now the entire sklearn workflow takes <5 minutes to run, 
#so it feels like wasted effort to break this into multiple rules 
rule sklearn_runs:
    input:
        net="data/{networks}.txt",
        netV="data/{networks}.txt_val",
        feat="data/{features}.tsv"
    params:
        curModel = "{sk_model}"
    output:
        "sk_runs/sk_{sk_model}-{networks}-{features}.p"
    shell:
        "python sckitModels.py {input.net} {input.netV} {input.feat} {params.curModel} {output};"

rule make_pytorch_datasets:
    input:
        net="data/{networks}.txt",
        netV="data/{networks}.txt_val",
        feat="data/{features}.tsv"
    output:
        full="torchDatasets/{networks}-{features}.p",
        val="torchDatasets/{networks}-{features}.p_val"
    shell:
        "python prepPytorchData.py {input.net} {input.feat} {output.full};"
        "python prepPytorchData.py {input.netV} {input.feat} {output.val}"

rule tune_pytorch_params:
    input:
        "torchDatasets/{networks}-{features}.p_val"
    params:
        curModel = "{model}"
    output:
        "axRuns/{model}-{networks}-{features}.json"
    shell:
        "python localizationTuningAx.py {params.curModel} {input} {output}"

rule pytorch_run:
    input:
        "axRuns/{model}-{networks}-{features}.json",
        "torchDatasets/{networks}-{features}.p"
    output:
        "runs/{model}-{networks}-{features}.p"
    shell:
        "python localizationPyTorchGeo.py {input} {output}"

rule analyze_results:
    input:
        expand("runs/{model}-{networks}-{features}.p", model=MODELS,networks=NETWORKS,features=FEATURES),
        expand("sk_runs/sk_{sk_model}-{networks}-{features}.p", sk_model=SKLEARN_MODELS,networks=NETWORKS,features=FEATURES),
        expand("pgm_runs/pgm_{pgm_model}-{networks}-{features}.p", pgm_model=PGM_MODELS,networks=NETWORKS,features=FEATURES)
    output:
        "results/allRes.p"
    shell:
        "python combineAnalyzeRes.py {input}"

rule all:
    input:
        "results/allRes.p"

