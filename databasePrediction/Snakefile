MODELS=['LinearNN', 'SimpleGCN', 'GATCONV', 'GIN2']
SKLEARN_MODELS=['logit','rf']
NETWORKS=['allReactomePaths','allPathBank']
NAIVE_FEATURES=['comPPINodes','compartmentsNodes']
FEATURES=['comPPINodes','compartmentsNodes','mergedKeyWords_5']

SKLEARN_MODELS=[]
#If pgm models have not been compiled, we can't run them
from os.path import isfile
if (isfile('pgm/train_nets') and isfile('pgm/no_training_model')):
    PGM_MODELS=['TrainedPGM','NaivePGM']
else:
    PGM_MODELS=[]

#This rule makes sure that all the data has been unzipped before running other steps.
rule unpack_data:
    output:
        unpackedProof = "data/data-is-unpacked.txt"
    shell:
        "bash unpackData.sh {output.unpackedProof}"

#The PGM code is tricky to install and uses c++, so it's not currently automatable. 
#Right now we just assume you have the compiled code in the right places. 
rule pgm_runs:
    input:
        unpacked = "data/data-is-unpacked.txt",
        net = "data/{networks}.txt",
        feat = "data/{features}.tsv"
    params:
        curModel = "{pgm_model}",
        outputDir = "pgm_runs/pgm_{pgm_model}-{networks}-{features}"
    output:
        full = "pgm_runs/pgm_{pgm_model}-{networks}-{features}.p"
    shell:
        "bash runPGM.sh {input.net} {input.feat} {params.curModel} {output.full} {params.outputDir};"

#The entire sklearn workflow takes <5 minutes to run, 
#so it feels like wasted effort to break this into multiple rules 
#
# Note: _val stands for validation and are generally random 10% subsets of the total dataset. 
rule sklearn_runs:
    input:
        unpacked = "data/data-is-unpacked.txt",
        net = "data/{networks}.txt",
        netV = "data/{networks}.txt_val",
        feat = "data/{features}.tsv"
    params:
        curModel = "{sk_model}"
    output:
        "sk_runs/sk_{sk_model}-{networks}-{features}.p"
    shell:
        "python sckitModels.py {input.net} {input.netV} {input.feat} {params.curModel} {output};"

#Take in network files, features, and ground-truth labels to create pytorch data objects. 
rule make_pytorch_datasets:
    input:
        unpacked = "data/data-is-unpacked.txt",
        net = "data/{networks}.txt",
        netV = "data/{networks}.txt_val",
        feat = "data/{features}.tsv"
    output:
        full = "torchDatasets/{networks}-{features}.p",
        val = "torchDatasets/{networks}-{features}.p_val"
    shell:
        "python prepPytorchData.py {input.net} {input.feat} {output.full};"
        "python prepPytorchData.py {input.netV} {input.feat} {output.val}"

#Perform parameter tuning using bayesian optimization
rule tune_pytorch_params:
    input:
        "torchDatasets/{networks}-{features}.p_val"
    params:
        curModel = "{model}"
    output:
        "axRuns/{model}-{networks}-{features}.json"
    shell:
        "python localizationTuningAx.py {params.curModel} {input} {output}"

#Perform pytorch run using best-performing parameters
rule pytorch_run:
    input:
        "axRuns/{model}-{networks}-{features}.json",
        "torchDatasets/{networks}-{features}.p"
    output:
        "runs/{model}-{networks}-{features}.p"
    shell:
        "python localizationPyTorchGeo.py {input} {output}"

#Combine results from different sources to produce a dictionary of metrics. 
rule analyze_results:
    input:
        expand("runs/{model}-{networks}-{features}.p", model=MODELS,networks=NETWORKS,features=FEATURES),
        expand("sk_runs/sk_{sk_model}-{networks}-{features}.p", sk_model=SKLEARN_MODELS,networks=NETWORKS,features=FEATURES),
        #expand("pgm_runs/pgm_TrainedPGM-{networks}-{features}.p", pgm_model=PGM_MODELS,networks=NETWORKS,features=FEATURES),
        #expand("pgm_runs/pgm_NaivePGM-{networks}-{features}.p", pgm_model=PGM_MODELS,networks=NETWORKS,features=NAIVE_FEATURES)
    output:
        "results/allRes.p"
    shell:
        "python combineAnalyzeRes.py {input}"

#Finally, create plots. 
rule all:
    input:
        res = "results/allRes.p"
    shell:
        "python plotResults.py {input.res}"

